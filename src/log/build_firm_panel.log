-----------------------------------------------------------------------------------------------------------------------------------------
      name:  <unnamed>
       log:  /Users/saul/Dropbox/Remote Work Startups/main/src/log/build_firm_panel.log
  log type:  text
 opened on:  26 Jun 2025, 19:26:55

. 
. 
. import delimited "$raw_data/Scoop_alt.csv", clear
(encoding automatically selected: ISO-8859-1)
(6 vars, 323,593 obs)

. 
. gen date_numeric = date(date, "YMD")

. drop date

. rename date_numeric date

. format date %td

. 
. gen yh = hofd(date)

. gen year = yofd(date)

. format yh %th

. 
. collapse (last) date (sum) join leave, by(companyname yh)

. 
. 
. keep companyname yh join leave

. tempfile join_leave

. save `join_leave'
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000001 saved as .dta format

. 
. 
. import delimited "$processed_data/Scoop_Positions_Firm_Collapse2.csv", clear
(encoding automatically selected: ISO-8859-1)
(6 vars, 323,675 obs)

. drop v1

. 
. gen date_numeric = date(date, "YMD")

. drop date

. rename date_numeric date

. format date %td

. 
. gen yh = hofd(date)

. gen year = yofd(date)

. format yh %th

. 
. // Drop one-off observations in June 2022
. drop if date == 22797
(4,345 observations deleted)

. 
. 
. // Collapse to have one observation per firm-half-year, and calculate growth & rates:
. collapse (last) total_employees date (sum) join leave, by(companyname yh)

. 
. drop join leave

. merge 1:1 companyname yh using `join_leave'

    Result                      Number of obs
    -----------------------------------------
    Not matched                            16
        from master                        14  (_merge==1)
        from using                          2  (_merge==2)

    Matched                            54,161  (_merge==3)
    -----------------------------------------

. drop _merge

. 
. encode companyname, gen(company_numeric)

. xtset company_numeric yh

Panel variable: company_numeric (unbalanced)
 Time variable: yh, 2016h1 to 2022h1, but with gaps
         Delta: 1 halfyear

. sort company_numeric yh

. 
. gen growth_rate = (total_employees / L.total_employees) - 1 if _n > 1
(4,422 missing values generated)

. gen join_rate = join / L.total_employees if _n > 1
(4,427 missing values generated)

. gen leave_rate = leave / L.total_employees if _n > 1
(4,427 missing values generated)

. 
. xtset, clear

. 
. winsor2 growth_rate join_rate leave_rate, cuts(1 99) suffix(_we)

. label variable growth_rate_we "Winsorized growth rate [1,99]"

. label variable join_rate_we "Winsorized join rate [1,99]"

. label variable leave_rate_we "Winsorized leave rate [1,99]"

. 
. drop growth_rate join_rate leave_rate company_numeric

. 
. 
. /*************************************************************************
>  * 4) Merge firm-level characteristics into worker-level data
>  *************************************************************************/
. 
. // Merge teleworkable data:
. merge m:1 companyname using "$processed_data/scoop_firm_tele_2.dta"

    Result                      Number of obs
    -----------------------------------------
    Not matched                           518
        from master                       517  (_merge==1)
        from using                          1  (_merge==2)

    Matched                            53,660  (_merge==3)
    -----------------------------------------

. drop if _merge == 2
(1 observation deleted)

. drop _merge

. 
. // Merge with flexibility measures (e.g., remote/flexibility scores):
. merge m:1 companyname using "$raw_data/Scoop_clean_public.dta"

    Result                      Number of obs
    -----------------------------------------
    Not matched                         5,787
        from master                     4,889  (_merge==1)
        from using                        898  (_merge==2)

    Matched                            49,288  (_merge==3)
    -----------------------------------------

. drop if _merge == 2
(898 observations deleted)

. drop _merge

. 
. // Merge with founding year data:
. merge m:1 companyname using "$raw_data/Scoop_founding.dta"

    Result                      Number of obs
    -----------------------------------------
    Not matched                            37
        from master                         0  (_merge==1)
        from using                         37  (_merge==2)

    Matched                            54,177  (_merge==3)
    -----------------------------------------

. drop if _merge == 2
(37 observations deleted)

. drop _merge

. 
. tempfile snapshot_clean

. save `snapshot_clean', replace
(file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000002 not found)
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000002 saved as .dta format

. 
. 
. * Merge Hierarchy Data (Centrality/HHI Analysis)
. use "$processed_data/Firm_role_level.dta", clear

. 
. keep companyname hhi_1000 seniority_levels

. 
. merge 1:m companyname using `snapshot_clean'

    Result                      Number of obs
    -----------------------------------------
    Not matched                           958
        from master                         0  (_merge==1)
        from using                        958  (_merge==2)

    Matched                            53,219  (_merge==3)
    -----------------------------------------

. drop if _merge == 1 
(0 observations deleted)

. drop _merge

. 
. save `snapshot_clean', replace
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000002 saved as .dta format

. 
. 
. * Merge Rent Data 
. use "$raw_data/data_20240523_lease.dta", clear

. drop id_Lease

. 
. gen half = ceil(execution_month/6)

. gen yh  = yh(execution_year, half)

. format yh %th

. 
. 
. keep if yh < yh(2020, 1)
(112,516 observations deleted)

. collapse (mean) effectiverent2212usdperyear [fweight=transactionsqft], by(city state)

. 
. 
. gen hqcity  = strtrim(city)

. gen hqstate = strtrim(state)

. sort hqcity hqstate

. egen hqID = group(hqcity hqstate), label

. label var hqID "Unique numeric ID of HQ city & state"

. 
. tempfile _lease

. save `_lease', replace
(file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000003 not found)
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000003 saved as .dta format

. 
. 
. use `snapshot_clean', clear 

. 
. merge m:1 hqcity hqstate using `_lease'
(variable hqcity was str25, now str40 to accommodate using data's values)

    Result                      Number of obs
    -----------------------------------------
    Not matched                        12,470
        from master                     8,307  (_merge==1)
        from using                      4,163  (_merge==2)

    Matched                            45,870  (_merge==3)
    -----------------------------------------

. drop if _merge == 2          
(4,163 observations deleted)

. drop _merge                  

. 
. rename effectiverent2212usdperyear rent

. 
. 
. * Merge firm modal role 
. merge m:1 companyname using "$processed_data/modal_role_per_firm.dta"

    Result                      Number of obs
    -----------------------------------------
    Not matched                           590
        from master                       589  (_merge==1)
        from using                          1  (_merge==2)

    Matched                            53,588  (_merge==3)
    -----------------------------------------

. drop if _merge == 2   
(1 observation deleted)

. drop _merge

. 
. 
. * Merge Wage dispersion 
. merge m:1 companyname using "$processed_data/wages_firm.dta"

    Result                      Number of obs
    -----------------------------------------
    Not matched                           590
        from master                       589  (_merge==1)
        from using                          1  (_merge==2)

    Matched                            53,588  (_merge==3)
    -----------------------------------------

. drop if _merge == 2    // drop observations that only exist in using data
(1 observation deleted)

. drop _merge

. 
. 
. gen age = 2020 - founded

. label var age "Firm age as of 2020"

. encode companyname, gen(firm_id)

. 
. 
. gen startup = (age <= 10)

. gen covid = yh >= 120

. 
. 
. rename flexibility_score2 remote

. 
. gen hybrid  = (remote>0 & remote<1)

. gen fullrem = (remote==1)

. 
. * For the hybrid treatment
. gen var3_hybrid = hybrid * covid

. gen var5_hybrid = hybrid * covid * startup

. 
. * For the fully-remote treatment
. gen var3_fullrem = fullrem * covid

. gen var5_fullrem = fullrem * covid * startup

. 
. 
. summarize yh

    Variable |        Obs        Mean    Std. dev.       Min        Max
-------------+---------------------------------------------------------
          yh |     54,177     118.138    3.727565        112        124

. local global_min = r(min)

. bys firm_id: egen min_time = min(yh)

. 
. preserve

.     contract yh, freq(count_yh)

.     local total_periods = _N

. restore

. 
. keep if min_time == `global_min'
(3,790 observations deleted)

. drop min_time

. 
end of do-file

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. import delimited "$raw_data/Scoop_workers_positions.csv", clear bindquote(strict) rowrange(0:1000) ///
>     stringcols(_all) 
0 is an invalid start row in rowrange() option
r(198);

end of do-file

r(198);

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. import delimited "$raw_data/Scoop_workers_positions.csv", clear bindquote(strict) rowrange(1:1000) ///
>     stringcols(_all) 
(encoding automatically selected: UTF-8)
(40 vars, 999 obs)

. 
end of do-file

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. import delimited "$raw_data/Scoop_workers_positions.csv", clear bindquote(strict) ///
>     stringcols(_all) 
(encoding automatically selected: UTF-8)
--Break--
r(1);

end of do-file

--Break--
r(1);

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. import delimited "$raw_data/Scoop_workers_positions.csv", clear rowrange(1:1000) bindquote(strict) ///
>     stringcols(_all) 
(encoding automatically selected: UTF-8)
(40 vars, 999 obs)

.     
. 
. tempfile tf_linkedin_full

. save `tf_linkedin_full', replace
(file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000001 not found)
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000001 saved as .dta format

. 
. import delimited "$raw_data/linkedin_msa_with_cbsa.csv", clear
(encoding automatically selected: ISO-8859-1)
(2 vars, 404 obs)

. 
. merge 1:m msa using `tf_linkedin_full'

    Result                      Number of obs
    -----------------------------------------
    Not matched                           316
        from master                       316  (_merge==1)
        from using                          0  (_merge==2)

    Matched                               999  (_merge==3)
    -----------------------------------------

. drop if _merge == 1
(316 observations deleted)

. 
end of do-file

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. import delimited "$raw_data/linkedin_msa_with_cbsa.csv", clear
(encoding automatically selected: ISO-8859-1)
(2 vars, 404 obs)

. 
end of do-file

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. import delimited "$raw_data/2024_Gaz_cbsa_national.txt", clear
(encoding automatically selected: ISO-8859-2)
(10 vars, 935 obs)

. 
end of do-file

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. *-------------------------------------------*
. * 0.  House-keeping
. *-------------------------------------------*
. clear all

. set more off

. 
. *-------------------------------------------*
. * 1.  Bring in the LinkedIn-MSA data
. *-------------------------------------------*
. import delimited "$raw_data/linkedin_msa_with_cbsa.csv", clear      // wide dataset
(encoding automatically selected: ISO-8859-1)
(2 vars, 404 obs)

. destring cbsacode, replace force   // make sure it's numeric
cbsacode already numeric; no replace

. 
. *-------------------------------------------*
. * 2.  Load the 2024 Gazetteer CBSA centroids
. *     (GEOID = 5-digit CBSA code)
. *-------------------------------------------*
. tempfile coords                                   // temp file for the lookup

. preserve

. import delimited "$raw_data/2024_Gaz_cbsa_national.txt", ///
>         delimiter("\t") varnames(1) clear
(encoding automatically selected: ISO-8859-2)
(10 vars, 935 obs)

. 
. keep GEOID INTPTLAT INTPTLONG                     // just what we need
variable GEOID not found
r(111);

end of do-file

r(111);

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. import delimited "$raw_data/2024_Gaz_cbsa_national.txt", ///
>         delimiter("\t") varnames(1) clear
(encoding automatically selected: ISO-8859-2)
(10 vars, 935 obs)

. 
end of do-file

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. keep GEOID INTPTLAT INTPTLONG                     // just what we need
variable GEOID not found
r(111);

end of do-file

r(111);

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. keep geoid intptlat intptlong                     // just what we need

. rename (geoid intptlat intptlong) (cbsacode lat lon)

. 
end of do-file

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. keep geoid intptlat intptlong                     // just what we need
variable geoid not found
r(111);

end of do-file

r(111);

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. *-------------------------------------------*
. * 0.  House-keeping
. *-------------------------------------------*
. clear all

. set more off

. 
. *-------------------------------------------*
. * 1.  Bring in the LinkedIn-MSA data
. *-------------------------------------------*
. import delimited "$raw_data/linkedin_msa_with_cbsa.csv", clear      // wide dataset
(encoding automatically selected: ISO-8859-1)
(2 vars, 404 obs)

. destring cbsacode, replace force   // make sure it's numeric
cbsacode already numeric; no replace

. 
. *-------------------------------------------*
. * 2.  Load the 2024 Gazetteer CBSA centroids
. *     (GEOID = 5-digit CBSA code)
. *-------------------------------------------*
. tempfile coords                                 

. preserve

. import delimited "$raw_data/2024_Gaz_cbsa_national.txt", ///
>         delimiter("\t") varnames(1) clear
(encoding automatically selected: ISO-8859-2)
(10 vars, 935 obs)

. 
. keep geoid intptlat intptlong                     // just what we need

. rename (geoid intptlat intptlong) (cbsacode lat lon)

. 
. destring cbsacode, replace                        // key numeric to match main data
cbsacode already numeric; no replace

. destring lat lon, replace ignore("+")             // strip leading "+" and convert
lat already numeric; no replace
lon already numeric; no replace

. 
. save `coords'
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000001 saved as .dta format

. restore

. 
. *-------------------------------------------*
. * 3.  Attach coordinates
. *-------------------------------------------*
. merge 1:1 cbsacode using `coords', keep(match master) nogen
variable cbsacode does not uniquely identify observations in the master data
r(459);

end of do-file

r(459);

. duplicates report cbsacode

Duplicates in terms of cbsacode

--------------------------------------
   Copies | Observations       Surplus
----------+---------------------------
        1 |          313             0
        2 |           44            22
        3 |            3             2
       44 |           44            43
--------------------------------------

. duplicates list cbsacode if _dup
_dup not found
r(111);

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. *-------------------------------------------*
. * 0.  House-keeping
. *-------------------------------------------*
. clear all

. set more off

. 
. *-------------------------------------------*
. * 1.  Bring in the LinkedIn-MSA data
. *-------------------------------------------*
. import delimited "$raw_data/linkedin_msa_with_cbsa.csv", clear      // wide dataset
(encoding automatically selected: ISO-8859-1)
(2 vars, 404 obs)

. destring cbsacode, replace force   // make sure it's numeric
cbsacode already numeric; no replace

. 
. *-------------------------------------------*
. * 2.  Load the 2024 Gazetteer CBSA centroids
. *     (GEOID = 5-digit CBSA code)
. *-------------------------------------------*
. tempfile coords                                 

. preserve

. import delimited "$raw_data/2024_Gaz_cbsa_national.txt", ///
>         delimiter("\t") varnames(1) clear
(encoding automatically selected: ISO-8859-2)
(10 vars, 935 obs)

. 
. keep geoid intptlat intptlong                     // just what we need

. rename (geoid intptlat intptlong) (cbsacode lat lon)

. 
. destring cbsacode, replace                        // key numeric to match main data
cbsacode already numeric; no replace

. destring lat lon, replace ignore("+")             // strip leading "+" and convert
lat already numeric; no replace
lon already numeric; no replace

. 
. save `coords'
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000001 saved as .dta format

. restore

. 
. *-------------------------------------------*
. * 3.  Attach coordinates
. *-------------------------------------------*
. merge m:1 cbsacode using `coords', keep(match master) nogen

    Result                      Number of obs
    -----------------------------------------
    Not matched                            44
        from master                        44  
        from using                          0  

    Matched                               360  
    -----------------------------------------

. 
. *-------------------------------------------*
. * 4.  Optional sanity-checks
. *-------------------------------------------*
. assert inrange(lat ,  18, 49)  // contiguous-US band
46 contradictions in 404 observations
assertion is false
r(9);

end of do-file

r(9);

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. *-------------------------------------------*
. * 0.  House-keeping
. *-------------------------------------------*
. clear all

. set more off

. 
. *-------------------------------------------*
. * 1.  Bring in the LinkedIn-MSA data
. *-------------------------------------------*
. import delimited "$raw_data/linkedin_msa_with_cbsa.csv", clear      // wide dataset
(encoding automatically selected: ISO-8859-1)
(2 vars, 404 obs)

. destring cbsacode, replace force   // make sure it's numeric
cbsacode already numeric; no replace

. 
. *-------------------------------------------*
. * 2.  Load the 2024 Gazetteer CBSA centroids
. *     (GEOID = 5-digit CBSA code)
. *-------------------------------------------*
. tempfile coords                                 

. preserve

. import delimited "$raw_data/2024_Gaz_cbsa_national.txt", ///
>         delimiter("\t") varnames(1) clear
(encoding automatically selected: ISO-8859-2)
(10 vars, 935 obs)

. 
. keep geoid intptlat intptlong                     // just what we need

. rename (geoid intptlat intptlong) (cbsacode lat lon)

. 
. destring cbsacode, replace                        // key numeric to match main data
cbsacode already numeric; no replace

. destring lat lon, replace ignore("+")             // strip leading "+" and convert
lat already numeric; no replace
lon already numeric; no replace

. 
. save `coords'
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000001 saved as .dta format

. restore

. 
. *-------------------------------------------*
. * 3.  Attach coordinates
. *-------------------------------------------*
. merge m:1 cbsacode using `coords'

    Result                      Number of obs
    -----------------------------------------
    Not matched                           643
        from master                        44  (_merge==1)
        from using                        599  (_merge==2)

    Matched                               360  (_merge==3)
    -----------------------------------------

. 
. 
end of do-file

. keep if _merge == 1
(959 observations deleted)

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. *-------------------------------------------*
. * 0.  House-keeping
. *-------------------------------------------*
. clear all

. set more off

. 
. *-------------------------------------------*
. * 1.  Bring in the LinkedIn-MSA data
. *-------------------------------------------*
. import delimited "$raw_data/linkedin_msa_with_cbsa.csv", clear      // wide dataset
(encoding automatically selected: ISO-8859-1)
(2 vars, 404 obs)

. destring cbsacode, replace force   // make sure it's numeric
cbsacode already numeric; no replace

. 
. *-------------------------------------------*
. * 2.  Load the 2024 Gazetteer CBSA centroids
. *     (GEOID = 5-digit CBSA code)
. *-------------------------------------------*
. tempfile coords                                 

. preserve

. import delimited "$raw_data/2024_Gaz_cbsa_national.txt", ///
>         delimiter("\t") varnames(1) clear
(encoding automatically selected: ISO-8859-2)
(10 vars, 935 obs)

. 
. keep geoid intptlat intptlong                     // just what we need

. rename (geoid intptlat intptlong) (cbsacode lat lon)

. 
. destring cbsacode, replace                        // key numeric to match main data
cbsacode already numeric; no replace

. destring lat lon, replace ignore("+")             // strip leading "+" and convert
lat already numeric; no replace
lon already numeric; no replace

. 
. save `coords'
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000001 saved as .dta format

. restore

. 
. *-------------------------------------------*
. * 3.  Attach coordinates
. *-------------------------------------------*
. merge m:1 cbsacode using `coords'

    Result                      Number of obs
    -----------------------------------------
    Not matched                           643
        from master                        44  (_merge==1)
        from using                        599  (_merge==2)

    Matched                               360  (_merge==3)
    -----------------------------------------

. drop if _merge == 1
(44 observations deleted)

. 
. 
end of do-file

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. *-------------------------------------------*
. * 0.  House-keeping
. *-------------------------------------------*
. clear all

. set more off

. 
. *-------------------------------------------*
. * 1.  Bring in the LinkedIn-MSA data
. *-------------------------------------------*
. import delimited "$raw_data/linkedin_msa_with_cbsa.csv", clear      // wide dataset
(encoding automatically selected: ISO-8859-1)
(2 vars, 404 obs)

. destring cbsacode, replace force   // make sure it's numeric
cbsacode already numeric; no replace

. 
. *-------------------------------------------*
. * 2.  Load the 2024 Gazetteer CBSA centroids
. *     (GEOID = 5-digit CBSA code)
. *-------------------------------------------*
. tempfile coords                                 

. preserve

. import delimited "$raw_data/2024_Gaz_cbsa_national.txt", ///
>         delimiter("\t") varnames(1) clear
(encoding automatically selected: ISO-8859-2)
(10 vars, 935 obs)

. 
. keep geoid intptlat intptlong                     // just what we need

. rename (geoid intptlat intptlong) (cbsacode lat lon)

. 
. destring cbsacode, replace                        // key numeric to match main data
cbsacode already numeric; no replace

. destring lat lon, replace ignore("+")             // strip leading "+" and convert
lat already numeric; no replace
lon already numeric; no replace

. 
. save `coords'
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000001 saved as .dta format

. restore

. 
. *-------------------------------------------*
. * 3.  Attach coordinates
. *-------------------------------------------*
. merge m:1 cbsacode using `coords'

    Result                      Number of obs
    -----------------------------------------
    Not matched                           643
        from master                        44  (_merge==1)
        from using                        599  (_merge==2)

    Matched                               360  (_merge==3)
    -----------------------------------------

. keep if _merge == 3
(643 observations deleted)

. 
. 
end of do-file

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. do "globals.do"

. global raw_data "../data/raw"

. global processed_data  "../data/processed"

. global results "../results/raw"

. global clean_results "../results/cleaned"

. 
. 
. 
. 
. 
end of do-file

. 
. *-------------------------------------------*
. * 0.  House-keeping
. *-------------------------------------------*
. clear all

. set more off

. 
. *-------------------------------------------*
. * 1.  Bring in the LinkedIn-MSA data
. *-------------------------------------------*
. import delimited "$raw_data/linkedin_msa_with_cbsa.csv", clear      // wide dataset
(encoding automatically selected: ISO-8859-1)
(2 vars, 404 obs)

. destring cbsacode, replace force   // make sure it's numeric
cbsacode already numeric; no replace

. 
. *-------------------------------------------*
. * 2.  Load the 2024 Gazetteer CBSA centroids
. *     (GEOID = 5-digit CBSA code)
. *-------------------------------------------*
. tempfile coords                                 

. preserve

. import delimited "$raw_data/2024_Gaz_cbsa_national.txt", ///
>         delimiter("\t") varnames(1) clear
(encoding automatically selected: ISO-8859-2)
(10 vars, 935 obs)

. 
. keep geoid intptlat intptlong                     // just what we need

. rename (geoid intptlat intptlong) (cbsacode lat lon)

. 
. destring cbsacode, replace                        // key numeric to match main data
cbsacode already numeric; no replace

. destring lat lon, replace ignore("+")             // strip leading "+" and convert
lat already numeric; no replace
lon already numeric; no replace

. 
. save `coords'
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000001 saved as .dta format

. restore

. 
. *-------------------------------------------*
. * 3.  Attach coordinates
. *-------------------------------------------*
. merge m:1 cbsacode using `coords'

    Result                      Number of obs
    -----------------------------------------
    Not matched                           643
        from master                        44  (_merge==1)
        from using                        599  (_merge==2)

    Matched                               360  (_merge==3)
    -----------------------------------------

. keep if _merge == 3
(643 observations deleted)

. drop _merge

. 
. save "$processed_data/enriched_msa.dta"
file ../data/processed/enriched_msa.dta saved

. 
. 
end of do-file

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. import delimited "$raw_data/Scoop_workers_positions.csv", clear rowrange(1:1000) bindquote(strict) ///
>     stringcols(_all) 
(encoding automatically selected: UTF-8)
(40 vars, 999 obs)

.     
. 
. merge m:1 msa using "$processed_data/enriched_msa.dta"

    Result                      Number of obs
    -----------------------------------------
    Not matched                           434
        from master                       152  (_merge==1)
        from using                        282  (_merge==2)

    Matched                               847  (_merge==3)
    -----------------------------------------

. 
end of do-file

. keep if _merge == 1
(1,129 observations deleted)

. use "/Users/saul/Dropbox/Remote Work Startups/main/data/processed/modal_msa_per_firm.dta", clear

. drop if msa == "empty"
(1,297 observations deleted)

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. do "globals.do"
file globals.do not found
r(601);

end of do-file

r(601);

. cd ..
/Users/saul/Dropbox/Remote Work Startups/main/data

. cd ..
/Users/saul/Dropbox/Remote Work Startups/main

. cd src
/Users/saul/Dropbox/Remote Work Startups/main/src

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. do "globals.do"

. global raw_data "../data/raw"

. global processed_data  "../data/processed"

. global results "../results/raw"

. global clean_results "../results/cleaned"

. 
. 
. 
. 
. 
end of do-file

. 
. 
. 
. ********************************************************************************
. * Import LinkedIn Occupation Data
. ********************************************************************************
. 
. import delimited "$raw_data/Scoop_workers_positions.csv", clear rowrange(1:1000) bindquote(strict) ///
>     stringcols(_all) 
(encoding automatically selected: UTF-8)
(40 vars, 999 obs)

.     
. merge m:1 msa using "$processed_data/enriched_msa.dta"

    Result                      Number of obs
    -----------------------------------------
    Not matched                           434
        from master                       152  (_merge==1)
        from using                        282  (_merge==2)

    Matched                               847  (_merge==3)
    -----------------------------------------

. keep if _merge == 3
(434 observations deleted)

. 
. 
. * Convert date strings to Stata date values.
. gen start = date(start_date, "YMD")
(233 missing values generated)

. gen end   = date(end_date, "YMD")

. format start %td

. format end   %td

. 
. 
. 
. local cutoff = date("2019-12-31", "YMD")

. keep if start <= `cutoff' & (missing(end) | end >= `cutoff')
(587 observations deleted)

. 
. 
. 
end of do-file

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. do "globals.do"

. global raw_data "../data/raw"

. global processed_data  "../data/processed"

. global results "../results/raw"

. global clean_results "../results/cleaned"

. 
. 
. 
. 
. 
end of do-file

. 
. 
. 
. ********************************************************************************
. * Import LinkedIn Occupation Data
. ********************************************************************************
. 
. import delimited "$raw_data/Scoop_workers_positions.csv", clear rowrange(1:1000) bindquote(strict) ///
>     stringcols(_all) 
(encoding automatically selected: UTF-8)
(40 vars, 999 obs)

.     
. merge m:1 msa using "$processed_data/enriched_msa.dta"

    Result                      Number of obs
    -----------------------------------------
    Not matched                           434
        from master                       152  (_merge==1)
        from using                        282  (_merge==2)

    Matched                               847  (_merge==3)
    -----------------------------------------

. keep if _merge == 3
(434 observations deleted)

. 
. 
. * Convert date strings to Stata date values.
. gen start = date(start_date, "YMD")
(233 missing values generated)

. gen end   = date(end_date, "YMD")

. format start %td

. format end   %td

. 
. 
. 
. local cutoff = date("2019-12-31", "YMD")

. keep if start <= `cutoff' & (missing(end) | end >= `cutoff')
(587 observations deleted)

. 
. keep companyname cbsacode lat lon

. 
. 
. 
. cap which geodist

. if _rc ssc install geodist   // haversine distance (km or miles)

. 
. 
. preserve

. keep worker_id cbsacode lat lon
variable worker_id not found
r(111);

end of do-file

r(111);

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. do "globals.do"

. global raw_data "../data/raw"

. global processed_data  "../data/processed"

. global results "../results/raw"

. global clean_results "../results/cleaned"

. 
. 
. 
. 
. 
end of do-file

. 
. 
. 
. ********************************************************************************
. * Import LinkedIn Occupation Data
. ********************************************************************************
. 
. import delimited "$raw_data/Scoop_workers_positions.csv", clear rowrange(1:1000) bindquote(strict) ///
>     stringcols(_all) 
(encoding automatically selected: UTF-8)
(40 vars, 999 obs)

.     
. merge m:1 msa using "$processed_data/enriched_msa.dta"

    Result                      Number of obs
    -----------------------------------------
    Not matched                           434
        from master                       152  (_merge==1)
        from using                        282  (_merge==2)

    Matched                               847  (_merge==3)
    -----------------------------------------

. keep if _merge == 3
(434 observations deleted)

. 
. 
. * Convert date strings to Stata date values.
. gen start = date(start_date, "YMD")
(233 missing values generated)

. gen end   = date(end_date, "YMD")

. format start %td

. format end   %td

. 
. 
. 
. local cutoff = date("2019-12-31", "YMD")

. keep if start <= `cutoff' & (missing(end) | end >= `cutoff')
(587 observations deleted)

. 
. keep user_id companyname cbsacode lat lon

. 
. 
. 
. cap which geodist

. if _rc ssc install geodist   // haversine distance (km or miles)

. 
. 
. preserve

. keep worker_id cbsacode lat lon
variable worker_id not found
r(111);

end of do-file

r(111);

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. do "globals.do"

. global raw_data "../data/raw"

. global processed_data  "../data/processed"

. global results "../results/raw"

. global clean_results "../results/cleaned"

. 
. 
. 
. 
. 
end of do-file

. 
. 
. 
. ********************************************************************************
. * Import LinkedIn Occupation Data
. ********************************************************************************
. 
. import delimited "$raw_data/Scoop_workers_positions.csv", clear rowrange(1:1000) bindquote(strict) ///
>     stringcols(_all) 
(encoding automatically selected: UTF-8)
(40 vars, 999 obs)

.     
. merge m:1 msa using "$processed_data/enriched_msa.dta"

    Result                      Number of obs
    -----------------------------------------
    Not matched                           434
        from master                       152  (_merge==1)
        from using                        282  (_merge==2)

    Matched                               847  (_merge==3)
    -----------------------------------------

. keep if _merge == 3
(434 observations deleted)

. 
. 
. * Convert date strings to Stata date values.
. gen start = date(start_date, "YMD")
(233 missing values generated)

. gen end   = date(end_date, "YMD")

. format start %td

. format end   %td

. 
. 
. 
. local cutoff = date("2019-12-31", "YMD")

. keep if start <= `cutoff' & (missing(end) | end >= `cutoff')
(587 observations deleted)

. 
. keep user_id companyname cbsacode lat lon

. 
. 
. 
. cap which geodist

. if _rc ssc install geodist   // haversine distance (km or miles)

. 
. 
. preserve

. keep user_id cbsacode lat lon

. bys user_id cbsacode: keep if _n==1
(7 observations deleted)

. tempfile base

. save `base'
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000002 saved as .dta format

. 
. 
. joinby user_id using `base', unmatched(none)   // self-join

. keep if cbsacode < cbsacode_using               // drop mirror duplicates
cbsacode_using not found
r(111);

end of do-file

r(111);

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. /**********************************************************************
> *  LinkedIn Job-Spell  |  Pair-wise MSA Distance (un-weighted)
> *  -------------------------------------------------------------------
> *  Requirements:  - geodist (SSC)          : ssc install geodist
> *                 - enriched_msa.dta       : must contain cbsacode lat lon
> *                 - globals.do             : defines $raw_data  $processed_data
> **********************************************************************/
. 
. ***********************************************
. * 0.  Globals, packages
. ***********************************************
. do "globals.do"

. global raw_data "../data/raw"

. global processed_data  "../data/processed"

. global results "../results/raw"

. global clean_results "../results/cleaned"

. 
. 
. 
. 
. 
end of do-file

. 
. cap which geodist

. if _rc ssc install geodist , replace

. 
. ***********************************************
. * 1.  Import & basic merges
. ***********************************************
. import delimited ///
>     "$raw_data/Scoop_workers_positions.csv", ///
>     clear bindquote(strict) stringcols(_all) rowrange(1:1000)
(encoding automatically selected: UTF-8)
(40 vars, 999 obs)

. 
. * merge MSA info (needs cbsacode lat lon in the using file)
. merge m:1 msa using "$processed_data/enriched_msa.dta", keep(match) nogen

    Result                      Number of obs
    -----------------------------------------
    Not matched                             0
    Matched                               847  
    -----------------------------------------

. 
. ***********************************************
. * 2.  Convert dates & keep spells that overlap 2019
. ***********************************************
. gen  start = date(start_date , "YMD")
(233 missing values generated)

. gen  end   = date(end_date   , "YMD")

. format start end %td

. 
. local cutoff = date("2019-12-31", "YMD")

. keep if start <= `cutoff' & (missing(end) | end >= `cutoff')
(587 observations deleted)

. 
. ***********************************************
. * 3.  Keep only variables we need
. ***********************************************
. keep user_id companyname cbsacode lat lon

. 
. ***********************************************
. * 4.  Build pair-wise MSA distance per user
. ***********************************************
. preserve

.     *–– one row per user–MSA (unique locations, no weighting)
.     keep user_id cbsacode lat lon

.     bys user_id cbsacode: keep if _n == 1
(7 observations deleted)

. 
.     tempfile base

.     save `base'
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000002 saved as .dta format

. 
.     *–– self-join to create every unordered pair inside each user
.     joinby user_id using `base', unmatched(none)

.     keep if cbsacode < cbsacode_using          // drop mirror duplicates
cbsacode_using not found
r(111);

end of do-file

r(111);

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. /**********************************************************************
> *  Company-level geographic dispersion  (un-weighted)
> *  ---------------------------------------------------------------
> *  - avgdist_co : average km between every unique pair of MSAs
> *                 in which the company has ≥1 LinkedIn spell
> *  - nmsa_co    : count of distinct MSAs for the company
> **********************************************************************/
. 
. * ------------------------------------------------------------------ *
. * 0.  pre-requisites
. * ------------------------------------------------------------------ *
. do "globals.do"

. global raw_data "../data/raw"

. global processed_data  "../data/processed"

. global results "../results/raw"

. global clean_results "../results/cleaned"

. 
. 
. 
. 
. 
end of do-file

. cap which geodist

. if _rc ssc install geodist , replace     // great-circle distance

. 
. * ------------------------------------------------------------------ *
. * 1.  import & merge MSA coordinates
. * ------------------------------------------------------------------ *
. import delimited "$raw_data/Scoop_workers_positions.csv", ///
>       clear bindquote(strict) stringcols(_all) rowrange(1:1000)
(encoding automatically selected: UTF-8)
(40 vars, 999 obs)

. 
. merge m:1 msa using "$processed_data/enriched_msa.dta", ///
>       keep(match) nogen                       // needs cbsacode lat lon

    Result                      Number of obs
    -----------------------------------------
    Not matched                             0
    Matched                               847  
    -----------------------------------------

. 
. * ------------------------------------------------------------------ *
. * 2.  keep spells overlapping calendar-year 2019
. * ------------------------------------------------------------------ *
. gen start = date(start_date,"YMD")
(233 missing values generated)

. gen end   = date(end_date  ,"YMD")

. format start end %td

. 
. local cutoff = date("2019-12-31","YMD")

. keep if start <= `cutoff' & (missing(end) | end >= `cutoff')
(587 observations deleted)

. 
. * ------------------------------------------------------------------ *
. * 3.  minimal variables & harmonise types
. * ------------------------------------------------------------------ *
. keep companyname user_id cbsacode lat lon

. destring cbsacode, replace force      // ensure numeric; encode if string
cbsacode already numeric; no replace

. 
. * ------------------------------------------------------------------ *
. * 4.  build file of UNIQUE company–MSA rows
. * ------------------------------------------------------------------ *
. preserve

.     bys companyname cbsacode: keep if _n==1   // one row per company–MSA
(214 observations deleted)

.     tempfile cmplist

.     save `cmplist'
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000002 saved as .dta format

. restore

. 
. * ------------------------------------------------------------------ *
. * 5.  self-join to create every unordered MSA pair within company
. * ------------------------------------------------------------------ *
. joinby companyname using `cmplist', unmatched(none) ///
>        suffix(_1 _2)                           // → cbsacode_1  cbsacode_2 …
option suffix() not allowed
r(198);

end of do-file

r(198);

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. do "globals.do"

. global raw_data "../data/raw"

. global processed_data  "../data/processed"

. global results "../results/raw"

. global clean_results "../results/cleaned"

. 
. 
. 
. 
. 
end of do-file

. 
. *-------------------------------------------*
. * 0.  House-keeping
. *-------------------------------------------*
. clear all

. set more off

. 
. *-------------------------------------------*
. * 1.  Bring in the LinkedIn-MSA data
. *-------------------------------------------*
. import delimited "$raw_data/linkedin_msa_with_cbsa.csv", clear      // wide dataset
(encoding automatically selected: ISO-8859-1)
(2 vars, 404 obs)

. destring cbsacode, replace force   // make sure it's numeric
cbsacode already numeric; no replace

. 
. *-------------------------------------------*
. * 2.  Load the 2024 Gazetteer CBSA centroids
. *     (GEOID = 5-digit CBSA code)
. *-------------------------------------------*
. tempfile coords                                 

. preserve

. import delimited "$raw_data/2024_Gaz_cbsa_national.txt", ///
>         delimiter("\t") varnames(1) clear
(encoding automatically selected: ISO-8859-2)
(10 vars, 935 obs)

. 
. keep geoid intptlat intptlong                     // just what we need

. rename (geoid intptlat intptlong) (cbsacode lat lon)

. 
. destring cbsacode, replace                        // key numeric to match main data
cbsacode already numeric; no replace

. destring lat lon, replace ignore("+")             // strip leading "+" and convert
lat already numeric; no replace
lon already numeric; no replace

. 
. save `coords'
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000001 saved as .dta format

. restore

. 
. *-------------------------------------------*
. * 3.  Attach coordinates
. *-------------------------------------------*
. merge m:1 cbsacode using `coords'

    Result                      Number of obs
    -----------------------------------------
    Not matched                           643
        from master                        44  (_merge==1)
        from using                        599  (_merge==2)

    Matched                               360  (_merge==3)
    -----------------------------------------

. keep if _merge == 3
(643 observations deleted)

. drop _merge

. 
. save "$processed_data/enriched_msa.dta"
file ../data/processed/enriched_msa.dta already exists
r(602);

end of do-file

r(602);

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. do "globals.do"

. global raw_data "../data/raw"

. global processed_data  "../data/processed"

. global results "../results/raw"

. global clean_results "../results/cleaned"

. 
. 
. 
. 
. 
end of do-file

. 
. *-------------------------------------------*
. * 0.  House-keeping
. *-------------------------------------------*
. clear all

. set more off

. 
. *-------------------------------------------*
. * 1.  Bring in the LinkedIn-MSA data
. *-------------------------------------------*
. import delimited "$raw_data/linkedin_msa_with_cbsa.csv", clear      // wide dataset
(encoding automatically selected: ISO-8859-1)
(2 vars, 404 obs)

. destring cbsacode, replace force   // make sure it's numeric
cbsacode already numeric; no replace

. 
. *-------------------------------------------*
. * 2.  Load the 2024 Gazetteer CBSA centroids
. *     (GEOID = 5-digit CBSA code)
. *-------------------------------------------*
. tempfile coords                                 

. preserve

. import delimited "$raw_data/2024_Gaz_cbsa_national.txt", ///
>         delimiter("\t") varnames(1) clear
(encoding automatically selected: ISO-8859-2)
(10 vars, 935 obs)

. 
. keep geoid intptlat intptlong                     // just what we need

. rename (geoid intptlat intptlong) (cbsacode lat lon)

. 
. destring cbsacode, replace                        // key numeric to match main data
cbsacode already numeric; no replace

. destring lat lon, replace ignore("+")             // strip leading "+" and convert
lat already numeric; no replace
lon already numeric; no replace

. 
. save `coords'
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000001 saved as .dta format

. restore

. 
. *-------------------------------------------*
. * 3.  Attach coordinates
. *-------------------------------------------*
. merge m:1 cbsacode using `coords'

    Result                      Number of obs
    -----------------------------------------
    Not matched                           643
        from master                        44  (_merge==1)
        from using                        599  (_merge==2)

    Matched                               360  (_merge==3)
    -----------------------------------------

. keep if _merge == 3
(643 observations deleted)

. drop _merge

. 
. save "$processed_data/enriched_msa.dta"
file ../data/processed/enriched_msa.dta already exists
r(602);

end of do-file

r(602);

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. do "globals.do"

. global raw_data "../data/raw"

. global processed_data  "../data/processed"

. global results "../results/raw"

. global clean_results "../results/cleaned"

. 
. 
. 
. 
. 
end of do-file

. 
. *-------------------------------------------*
. * 0.  House-keeping
. *-------------------------------------------*
. clear all

. set more off

. 
. *-------------------------------------------*
. * 1.  Bring in the LinkedIn-MSA data
. *-------------------------------------------*
. import delimited "$raw_data/linkedin_msa_with_cbsa.csv", clear      // wide dataset
(encoding automatically selected: ISO-8859-1)
(2 vars, 404 obs)

. destring cbsacode, replace force   // make sure it's numeric
cbsacode already numeric; no replace

. 
. *-------------------------------------------*
. * 2.  Load the 2024 Gazetteer CBSA centroids
. *     (GEOID = 5-digit CBSA code)
. *-------------------------------------------*
. tempfile coords                                 

. preserve

. import delimited "$raw_data/2024_Gaz_cbsa_national.txt", ///
>         delimiter("\t") varnames(1) clear
(encoding automatically selected: ISO-8859-2)
(10 vars, 935 obs)

. 
. keep geoid intptlat intptlong                     // just what we need

. rename (geoid intptlat intptlong) (cbsacode lat lon)

. 
. destring cbsacode, replace                        // key numeric to match main data
cbsacode already numeric; no replace

. destring lat lon, replace ignore("+")             // strip leading "+" and convert
lat already numeric; no replace
lon already numeric; no replace

. 
. save `coords'
file /var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//S_22469.000001 saved as .dta format

. restore

. 
. *-------------------------------------------*
. * 3.  Attach coordinates
. *-------------------------------------------*
. merge m:1 cbsacode using `coords'

    Result                      Number of obs
    -----------------------------------------
    Not matched                           643
        from master                        44  (_merge==1)
        from using                        599  (_merge==2)

    Matched                               360  (_merge==3)
    -----------------------------------------

. keep if _merge == 3
(643 observations deleted)

. drop _merge

. 
. save "$processed_data/enriched_msa.dta", replace
file ../data/processed/enriched_msa.dta saved

. export delimited "$processed_data/enriched_msa.csv", replace
(file ../data/processed/enriched_msa.csv not found)
file ../data/processed/enriched_msa.csv saved

. 
. 
end of do-file

. do "/var/folders/hg/s3xm9w393gxby64_zklmhgbc0000gn/T//SD22469.000000"

. /*-------------------------------------------------------------------
> | build_user_panel.do — Generate user-level panel datasets
> | Author : <your-name>
> | Updated: 06 Jun 2025
> |
> | DESCRIPTION
> | ----------
> | Re-runs the *original* cleaning / merging pipeline **unchanged**, then
> | branches into three sample variants **without touching any other
> | logic**:
> |   • unbalanced – full cleaned panel (default)
> |   • balanced   – users observed in *every* half-year between the
> |                  global min & max yh
> |   • precovid   – users with positive pre-COVID restricted contributions
> |
> # NOTE ──────────────────────────────────────────────────────────────────
> # In earlier versions the *pre-COVID* (“precovid”) sample was silently
> # duplicated to the generic legacy filenames `user_panel.dta/csv`.  This
> # implicit fallback made it impossible to see at a glance which panel
> # variant later specification scripts had been run on.
> #
> # The compatibility artefact has now been **removed**: every output file
> # is written *only* under an explicit, self-describing filename of the
> # form `user_panel_<variant>.dta|csv` (e.g. `user_panel_unbalanced.dta`).
> # Down-stream code must therefore always reference the panel variant
> # explicitly in filenames (e.g. by passing it as an argument to
> # specification scripts).  No more silent defaults.
> |
> | USAGE
> | -----
> |   do build_user_panel.do
> |   *Optionally* edit `local sample_types` to generate a subset.
> *-------------------------------------------------------------------*/
. 
. capture log close
